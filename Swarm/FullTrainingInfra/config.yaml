# RL Training Configuration
experiment:
  name: "quantum_device_multiagent_ppo"
  project: "qubit-array-tuning"
  tags: ["multi-agent", "ppo", "quantum-dots"]
  seed: 42

# Environment Configuration
env:
  name: "QuantumDeviceEnv"
  num_envs: 16  # Number of parallel environments
  use_gpu_for_rollouts: false  # Switch between GPU/CPU for environment rollouts
  rollout_fragment_length: 200
  train_batch_size: 4000
  sgd_minibatch_size: 128

# Multi-Agent Configuration
multi_agent:
  policies:
    plunger_policy:
      obs_space: null  # Will be set dynamically
      action_space: null  # Will be set dynamically
    barrier_policy:
      obs_space: null  # Will be set dynamically
      action_space: null  # Will be set dynamically
  
  # Policy mapping function will map agent IDs to policies
  policy_mapping_mode: "by_agent_type"  # plunger vs barrier

# PPO Algorithm Configuration
ppo:
  # Learning rate
  lr: 3e-4
  lr_schedule: null
  
  # PPO-specific hyperparameters
  clip_param: 0.2
  vf_clip_param: 10.0
  entropy_coeff: 0.01
  vf_loss_coeff: 0.5
  kl_coeff: 0.2
  kl_target: 0.01
  
  # Training configuration
  num_sgd_iter: 10
  gamma: 0.99
  lambda: 0.95
  
  # Neural network configuration
  model:
    fcnet_hiddens: [256, 256]
    fcnet_activation: "tanh"
    use_lstm: false
    max_seq_len: 20
    lstm_cell_size: 256
    lstm_use_prev_action: false
    lstm_use_prev_reward: false
    
    # For image observations (scans)
    conv_filters: null  # Will be set if needed
    conv_activation: "relu"
    post_fcnet_hiddens: []
    post_fcnet_activation: "relu"

# Ray Configuration
ray:
  # Resource allocation
  num_gpus: 8
  num_cpus_per_worker: 1
  num_gpus_per_worker: 0.125  # 8 workers per GPU
  num_workers: 64  # Total number of rollout workers
  
  # Advanced Ray settings
  batch_mode: "complete_episodes"
  remote_worker_envs: true
  remote_env_batch_wait_ms: 0
  
  # Memory and performance
  object_store_memory: 10000000000  # 10GB
  
# Logging Configuration
logging:
  wandb:
    enabled: true
    api_key_env: "WANDB_API_KEY"
    
  # Custom metrics to track
  custom_metrics:
    - "episode_reward_mean"
    - "episode_len_mean"
    - "policy_loss"
    - "vf_loss"
    - "entropy"
    - "kl_divergence"
    - "plunger_agent_rewards"
    - "barrier_agent_rewards"
    - "individual_agent_rewards"
    
  # Logging frequency
  log_frequency: 10  # Log every 10 training iterations
  
# Checkpointing
checkpointing:
  frequency: 100  # Save checkpoint every 100 iterations
  keep_checkpoints_num: 5
  checkpoint_score_attr: "episode_reward_mean"
  
# Training Stop Conditions
stopping_criteria:
  training_iteration: 10000
  episode_reward_mean: 1000  # Stop if mean reward reaches this threshold
  timesteps_total: 10000000

# Evaluation
evaluation:
  evaluation_interval: 50
  evaluation_duration: 10  # Number of episodes for evaluation
  evaluation_parallel_to_training: true 