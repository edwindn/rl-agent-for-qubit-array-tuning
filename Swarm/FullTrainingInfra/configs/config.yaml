# RL Training Configuration
experiment:
  name: "quantum_device_multiagent_ppo"
  project: "qubit-array-tuning"
  tags: ["multi-agent", "ppo", "quantum-dots"]
  seed: 42

# Trainer Configuration
trainer_type: "recurrent_ppo"  # Options: ppo, recurrent_ppo, ppo_v2, sac (when implemented)

# Environment Configuration
env:
  name: "QuantumDeviceEnv"
  num_envs: 16  # Number of parallel environments
  use_gpu_for_rollouts: false  # Switch between GPU/CPU for environment rollouts
  rollout_fragment_length: 125
  train_batch_size: 2000
  mini_batch_size: 128

# Multi-Agent Configuration
multi_agent:
  policies:
    plunger_policy:
      obs_space: null  # Will be set dynamically
      action_space: null  # Will be set dynamically
    barrier_policy:
      obs_space: null  # Will be set dynamically
      action_space: null  # Will be set dynamically
  
  # Policy mapping function will map agent IDs to policies
  policy_mapping_mode: "by_agent_type"  # plunger vs barrier

# PPO Hyperparameter Overrides (optional)
# These override defaults from VoltageAgent/ppo_config.py
ppo_overrides:
  # Uncomment to override specific hyperparameters:
  # lr: 5e-4
  # clip_param: 0.3
  # model:
  #   fcnet_hiddens: [512, 512]

# Ray Configuration
ray:
  # Resource allocation
  num_gpus: 8
  num_cpus_per_worker: 1
  num_gpus_per_worker: 0.125  # 8 workers per GPU
  num_workers: 16  # num_env_runners
  num_envs_per_env_runner: 1
  
  # Advanced Ray settings
  batch_mode: "complete_episodes"
  remote_worker_envs: false
  remote_env_batch_wait_ms: 0
  
  # Memory and performance
  object_store_memory: 10000000000  # 10GB
  
# Logging Configuration
logging:
  wandb:
    enabled: true
    api_key_env: "WANDB_API_KEY"
    
  # Custom metrics to track
  custom_metrics:
    - "episode_reward_mean"
    - "episode_len_mean"
    - "policy_loss"
    - "vf_loss"
    - "entropy"
    - "kl_divergence"
    - "plunger_agent_rewards"
    - "barrier_agent_rewards"
    - "individual_agent_rewards"
    
  # Logging frequency
  log_frequency: 10  # Log every 10 training iterations
  
# Checkpointing
checkpointing:
  frequency: 100  # Save checkpoint every 100 iterations
  keep_checkpoints_num: 5
  checkpoint_score_attr: "episode_reward_mean"
  
# Training Stop Conditions
stopping_criteria:
  training_iteration: 10000
  episode_reward_mean: 1000  # Stop if mean reward reaches this threshold
  timesteps_total: 10000000

# Evaluation
evaluation:
  evaluation_interval: 50
  evaluation_duration: 10  # Number of episodes for evaluation
  evaluation_parallel_to_training: false # set to true with `evaluation_num_env_runners > 0`