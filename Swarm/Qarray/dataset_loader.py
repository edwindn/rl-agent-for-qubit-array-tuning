#!/usr/bin/env python3
"""
Dataset Loader for Quantum Device Datasets

This module provides utilities to load and work with quantum device datasets
generated by the dataset_generator.py script.
"""

import json
import numpy as np
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Generator, Any
import yaml


class QuantumDeviceDataset:
    """
    A class to load and manipulate quantum device datasets.
    
    This class provides methods to load datasets generated by the dataset generator,
    access images and parameters, and create train/validation/test splits.
    """
    
    def __init__(self, dataset_path: str):
        """
        Initialize the dataset loader.
        
        Args:
            dataset_path (str): Path to the dataset directory
        """
        self.dataset_path = Path(dataset_path)
        self.images_dir = self.dataset_path / 'images'
        self.parameters_dir = self.dataset_path / 'parameters'
        self.metadata_dir = self.dataset_path / 'metadata'
        
        # Validate dataset structure
        self._validate_dataset()
        
        # Load metadata
        self.metadata = self._load_metadata()
        self.dataset_info = self._load_dataset_info()
        
        # Discover available batches
        self.batch_files = self._discover_batches()
        self.total_samples = self._count_total_samples()
        
    def _validate_dataset(self) -> None:
        """Validate that the dataset directory has the expected structure"""
        required_dirs = [self.images_dir, self.parameters_dir, self.metadata_dir]
        for dir_path in required_dirs:
            if not dir_path.exists():
                raise FileNotFoundError(f"Required directory not found: {dir_path}")
    
    def _load_metadata(self) -> Dict[str, Any]:
        """Load dataset metadata"""
        metadata_path = self.metadata_dir / 'dataset_info.json'
        if metadata_path.exists():
            with open(metadata_path, 'r') as f:
                return json.load(f)
        return {}
    
    def _load_dataset_info(self) -> Dict[str, Any]:
        """Load dataset generation progress info"""
        progress_path = self.metadata_dir / 'progress.json'
        if progress_path.exists():
            with open(progress_path, 'r') as f:
                return json.load(f)
        return {}
    
    def _discover_batches(self) -> List[str]:
        """Discover available batch files"""
        image_files = sorted(self.images_dir.glob('batch_*.npy'))
        param_files = sorted(self.parameters_dir.glob('batch_*.npy'))
        
        # Ensure we have matching image and parameter files
        image_names = {f.stem for f in image_files}
        param_names = {f.stem for f in param_files}
        
        if image_names != param_names:
            missing_images = param_names - image_names
            missing_params = image_names - param_names
            if missing_images:
                print(f"Warning: Missing image files for: {missing_images}")
            if missing_params:
                print(f"Warning: Missing parameter files for: {missing_params}")
        
        # Return intersection of available files
        available_batches = sorted(image_names & param_names)
        return available_batches
    
    def _count_total_samples(self) -> int:
        """Count total number of samples in the dataset"""
        total = 0
        for batch_name in self.batch_files:
            image_path = self.images_dir / f'{batch_name}.npy'
            try:
                # Load just the shape without loading full array
                images = np.load(image_path, mmap_mode='r')
                total += images.shape[0]
            except Exception as e:
                print(f"Warning: Could not count samples in {batch_name}: {e}")
        return total
    
    def get_batch(self, batch_index: int) -> Tuple[np.ndarray, List[Dict[str, Any]]]:
        """
        Load a specific batch of data.
        
        Args:
            batch_index (int): Index of the batch to load
            
        Returns:
            Tuple[np.ndarray, List[Dict]]: Images and parameters for the batch
        """
        if batch_index >= len(self.batch_files):
            raise IndexError(f"Batch index {batch_index} out of range (0-{len(self.batch_files)-1})")
        
        batch_name = self.batch_files[batch_index]
        
        # Load images and parameters
        image_path = self.images_dir / f'{batch_name}.npy'
        param_path = self.parameters_dir / f'{batch_name}.npy'
        
        images = np.load(image_path)
        parameters = np.load(param_path, allow_pickle=True)
        
        return images, parameters.tolist()
    
    def get_sample(self, global_index: int) -> Tuple[np.ndarray, Dict[str, Any]]:
        """
        Get a specific sample by global index.
        
        Args:
            global_index (int): Global index of the sample across all batches
            
        Returns:
            Tuple[np.ndarray, Dict]: Image and parameters for the sample
        """
        if global_index >= self.total_samples:
            raise IndexError(f"Sample index {global_index} out of range (0-{self.total_samples-1})")
        
        # Find which batch contains this sample
        current_index = 0
        for batch_index, batch_name in enumerate(self.batch_files):
            image_path = self.images_dir / f'{batch_name}.npy'
            images = np.load(image_path, mmap_mode='r')
            batch_size = images.shape[0]
            
            if current_index + batch_size > global_index:
                # Sample is in this batch
                local_index = global_index - current_index
                images, parameters = self.get_batch(batch_index)
                return images[local_index], parameters[local_index]
            
            current_index += batch_size
        
        raise IndexError(f"Could not find sample {global_index}")
    
    def iterate_samples(self, batch_size: Optional[int] = None) -> Generator[Tuple[np.ndarray, Dict[str, Any]], None, None]:
        """
        Iterate over all samples in the dataset.
        
        Args:
            batch_size (Optional[int]): If provided, yield batches of this size instead of individual samples
            
        Yields:
            Tuple[np.ndarray, Dict]: Image and parameters for each sample (or batch of samples)
        """
        if batch_size is None:
            # Yield individual samples
            for batch_index in range(len(self.batch_files)):
                images, parameters = self.get_batch(batch_index)
                for i in range(len(images)):
                    yield images[i], parameters[i]
        else:
            # Yield batches of specified size
            current_batch = []
            current_params = []
            
            for batch_index in range(len(self.batch_files)):
                images, parameters = self.get_batch(batch_index)
                for i in range(len(images)):
                    current_batch.append(images[i])
                    current_params.append(parameters[i])
                    
                    if len(current_batch) >= batch_size:
                        yield np.array(current_batch), current_params
                        current_batch = []
                        current_params = []
            
            # Yield remaining samples if any
            if current_batch:
                yield np.array(current_batch), current_params
    
    def create_train_val_test_split(self, train_ratio: float = 0.7, val_ratio: float = 0.15, 
                                   test_ratio: float = 0.15, seed: int = 42) -> Dict[str, np.ndarray]:
        """
        Create train/validation/test splits and save indices.
        
        Args:
            train_ratio (float): Proportion of data for training
            val_ratio (float): Proportion of data for validation
            test_ratio (float): Proportion of data for testing
            seed (int): Random seed for reproducible splits
            
        Returns:
            Dict[str, np.ndarray]: Dictionary with 'train', 'val', 'test' indices
        """
        if abs(train_ratio + val_ratio + test_ratio - 1.0) > 1e-6:
            raise ValueError("Ratios must sum to 1.0")
        
        # Create random permutation of indices
        rng = np.random.default_rng(seed)
        indices = rng.permutation(self.total_samples)
        
        # Calculate split points
        train_end = int(train_ratio * self.total_samples)
        val_end = train_end + int(val_ratio * self.total_samples)
        
        # Create splits
        splits = {
            'train': indices[:train_end],
            'val': indices[train_end:val_end],
            'test': indices[val_end:]
        }
        
        # Save splits to file
        indices_dir = self.dataset_path / 'indices'
        indices_dir.mkdir(exist_ok=True)
        
        for split_name, split_indices in splits.items():
            np.save(indices_dir / f'{split_name}_indices.npy', split_indices)
        
        # Save split metadata
        split_info = {
            'train_ratio': train_ratio,
            'val_ratio': val_ratio,
            'test_ratio': test_ratio,
            'seed': seed,
            'total_samples': self.total_samples,
            'train_size': len(splits['train']),
            'val_size': len(splits['val']),
            'test_size': len(splits['test'])
        }
        
        with open(indices_dir / 'split_info.json', 'w') as f:
            json.dump(split_info, f, indent=2)
        
        return splits
    
    def load_splits(self) -> Optional[Dict[str, np.ndarray]]:
        """
        Load existing train/validation/test splits.
        
        Returns:
            Optional[Dict[str, np.ndarray]]: Split indices or None if not found
        """
        indices_dir = self.dataset_path / 'indices'
        
        if not indices_dir.exists():
            return None
        
        splits = {}
        for split_name in ['train', 'val', 'test']:
            split_path = indices_dir / f'{split_name}_indices.npy'
            if split_path.exists():
                splits[split_name] = np.load(split_path)
        
        return splits if len(splits) == 3 else None
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        Compute and return dataset statistics.
        
        Returns:
            Dict[str, Any]: Dataset statistics
        """
        # Load a few samples to compute statistics
        sample_count = min(1000, self.total_samples)
        images = []
        
        sample_indices = np.linspace(0, self.total_samples - 1, sample_count, dtype=int)
        
        for idx in sample_indices:
            image, _ = self.get_sample(idx)
            images.append(image)
        
        images = np.array(images)
        
        stats = {
            'total_samples': self.total_samples,
            'num_batches': len(self.batch_files),
            'image_shape': images[0].shape,
            'image_dtype': str(images.dtype),
            'image_min': float(np.min(images)),
            'image_max': float(np.max(images)),
            'image_mean': float(np.mean(images)),
            'image_std': float(np.std(images)),
            'dataset_size_mb': sum(
                (self.images_dir / f'{batch}.npy').stat().st_size + 
                (self.parameters_dir / f'{batch}.npy').stat().st_size
                for batch in self.batch_files
            ) / (1024 * 1024)
        }
        
        return stats
    
    def __len__(self) -> int:
        """Return the total number of samples in the dataset"""
        return self.total_samples
    
    def __repr__(self) -> str:
        """String representation of the dataset"""
        return (f"QuantumDeviceDataset(path='{self.dataset_path}', "
                f"samples={self.total_samples}, batches={len(self.batch_files)})")


def load_dataset(dataset_path: str) -> QuantumDeviceDataset:
    """
    Convenience function to load a quantum device dataset.
    
    Args:
        dataset_path (str): Path to the dataset directory
        
    Returns:
        QuantumDeviceDataset: Loaded dataset object
    """
    return QuantumDeviceDataset(dataset_path)


# Example usage
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Explore quantum device dataset')
    parser.add_argument('dataset_path', help='Path to dataset directory')
    parser.add_argument('--stats', action='store_true', help='Show dataset statistics')
    parser.add_argument('--create_splits', action='store_true', help='Create train/val/test splits')
    parser.add_argument('--show_sample', type=int, help='Show a specific sample')
    
    args = parser.parse_args()
    
    # Load dataset
    dataset = load_dataset(args.dataset_path)
    print(f"Loaded dataset: {dataset}")
    
    if args.stats:
        print("\nDataset Statistics:")
        stats = dataset.get_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")
    
    if args.create_splits:
        print("\nCreating train/validation/test splits...")
        splits = dataset.create_train_val_test_split()
        for split_name, indices in splits.items():
            print(f"  {split_name}: {len(indices)} samples")
    
    if args.show_sample is not None:
        print(f"\nSample {args.show_sample}:")
        image, params = dataset.get_sample(args.show_sample)
        print(f"  Image shape: {image.shape}")
        print(f"  Image range: [{image.min():.4f}, {image.max():.4f}]")
        print(f"  Voltage center: {params.get('voltage_center', 'N/A')}")
        print(f"  Seed: {params.get('seed', 'N/A')}") 