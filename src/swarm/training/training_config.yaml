# Training Configuration for Multi-Agent RL Quantum Device Tuning
# This file contains all configurable parameters for the training pipeline

# Default training parameters
defaults:
  num_iterations: 200        # Total training iterations
  num_gpus: 8                # Default number of GPUs to use

# Weights & Biases configuration
wandb:
  entity: "rl_agents_for_tuning"
  project: "RLModel"
  
  log_images: false        # Whether to log environment images to Wandb

# Ray cluster configuration
ray:
  include_dashboard: false   # Disable Ray dashboard for cleaner output
  log_to_driver: false      # Reduce driver logs
  logging_level: 30         # WARNING level (10=DEBUG, 20=INFO, 30=WARNING, 40=ERROR)
  
  # Runtime environment settings
  runtime_env:
    excludes:
      - "dataset"
      - "dataset_v1" 
      - "wandb"
      - "outputs"
      - "test_outputs"
      - "checkpoints"
      - "weights*"
      - "*dataset*"
    
    env_vars:
      JAX_PLATFORM_NAME: "cuda"
      JAX_PLATFORMS: "cuda"
      PYTHONWARNINGS: "ignore::DeprecationWarning"
      RAY_DEDUP_LOGS: "0"
      RAY_DISABLE_IMPORT_WARNING: "1"

# Algorithm Configuration
rl_config:
  algorithm: "PPO" # PPO or SAC

  # Environment runners configuration
  env_runners:
    num_env_runners: 4              # Number of parallel environment runners
    rollout_fragment_length: 50     # Steps per rollout fragment
    sample_timeout_s: 1200.0         # Timeout for sampling
    num_gpus_per_env_runner: 0.6    # GPU allocation per runner

  # Learner configuration  
  learners:
    num_learners: 1                 # Number of learner workers
    num_gpus_per_learner: 0.75      # GPU allocation per learner

  # Training hyperparameters
  training:
    train_batch_size: 4096          # Total batch size for training
    minibatch_size: 128             # Size of minibatches for SGD
    num_epochs: 2                   # Number of SGD epochs per training iteration
    grad_clip: 10.0                 # Amount of gradient clipping (set to null to disable) ~40.0 is the default
    grad_clip_by: "norm"            # Gradient clipping method

    # PPO-specific config
    lr: 0.0003                      # Learning rate
    gamma: 0.99                     # Discount factor
    lambda_: 0.95                   # GAE lambda parameter
    clip_param: 0.2                 # PPO clipping parameter
    entropy_coeff: 0.01             # Entropy regularization coefficient
    vf_loss_coeff: 0.5              # Value function loss coefficient
    kl_target: 0.01                 # Target size of KL penalty on policy updates

    # SAC-specific config
    actor_lr: 0.0003
    critic_lr: 0.0003
    alpha_lr: 0.0003
    twin_q: true                   # Whether to use twin Q-networks
    tau: 0.005                     # Target network update rate
    initial_alpha: 1.0              # Initial entropy coefficient 
    target_entropy: "auto"       # Target entropy ("auto" to use default based on action space)
    n_step: 1                # Number of steps to look ahead for multi-step returns
    clip_actions: true          # Whether to clip actions to the action space bounds
    target_network_update_freq: 1  # Frequency of target network updates (in training steps)
    num_steps_sampled_before_learning_starts: 1000  # Number of steps to sample before starting learning
    replay_buffer_config: {
      "type": "MultiAgentPrioritizedEpisodeReplayBuffer",
      "capacity": 1000000,                        # Replay buffer capacity
      "alpha": 0.6,                               # Prioritization exponent
      "beta": 0.4,                                # Importance sampling exponent
    }


  # Multi-agent configuration
  multi_agent:
    policies:
      - "plunger_policy"
      - "barrier_policy"
    policies_to_train:
      - "plunger_policy" 
      - "barrier_policy"
    count_steps_by: "agent_steps"

    free_log_std: False         #Â Whether to predict the std (better exploration) or not (more stable)
                                # Note this will affect how we extract the log std for logging
    log_std_bounds: [-10, 2]    # Ranges within which to clip predicted log_std for the policy head

# Resource allocation
resources:
  num_gpus: 8                       # Total GPUs for the algorithm

# Checkpoint configuration
checkpoints:
  save_dir: "./checkpoints"         # Directory to save checkpoints
  upload_best_only: true           # Only upload checkpoints with improved performance

# Neural Network Architecture Configuration
neural_networks:
  # chosee between SimpleCNN or IMPALA
  plunger_policy:
    backbone:
      type: "IMPALA"
      conv_layers:
        - {channels: 16, kernel: 4, stride: 2}
        - {channels: 32, kernel: 3, stride: 2}
        - {channels: 64, kernel: 3, stride: 1}
      feature_size: 512
      adaptive_pooling: true
      lstm:
        enabled: true
        cell_size: 512
        num_layers: 1
        max_seq_len: 50
        use_prev_action: true
        use_prev_reward: false
    policy_head:
      hidden_layers: [64, 64]  
      activation: "relu"
      use_attention: true
    value_head:
      hidden_layers: [512, 64]
      activation: "relu"
      use_attention: true

  barrier_policy:
    backbone:
      type: "SimpleCNN"
      conv_layers:
        - {channels: 8, kernel: 3, stride: 1}
      feature_size: 128
      adaptive_pooling: true
      # No lstm section = no LSTM for barriers
    policy_head:
      hidden_layers: [32, 32]  
      activation: "relu"
      use_attention: false
    value_head:
      hidden_layers: [32, 32]
      activation: "relu"
      use_attention: false