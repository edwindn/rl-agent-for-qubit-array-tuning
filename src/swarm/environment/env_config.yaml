# Env configuration
# Contains all configs related to the environment and the rl algorithm

simulator:
  # Max steps in a single training rollout
  max_steps: 50

  num_dots: 4

  use_barriers: false

  use_deltas: false # Whether the plunger agents' output is a Î”V or V
  
  delta_max: 2.0 #only used if use_deltas is true
  
  full_plunger_range_width: {"min": 15, "max": 20} #range in V of plungers

  window_delta_range: {"min": 1.0, "max": 2.0} #range in V for a single scan/sweep

  full_barrier_range_width: {"min": 6, "max": 10} #range in V of barriers

  resolution: 100


reward:
  plunger_reward_window_size: 20 # window size normalisation for plunger reward calculation

  barrier_reward_window_size: 10 #same for barrier
  
  gate_reward_exp: 2.0 # exponent for plunger reward
  
  breadcrumb_reward_factor: 1. # Factor between 'terminal' and 'approach' rewards

  tolerance: 0.1 #distance from ground truth to cap at max reward


capacitance_model:
  update_method: "fake"  # Options: null (does not update vgm), "fake" (for testing/debugging), "bayesian" (simple prior), "kriging" (history based spatial prior)

init:
  debug: false
  seed: 42